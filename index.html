<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Frutiger Aero Speech Cam (Multi-User)</title>
    
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>

    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #004e92, #000428);
            color: white;
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 0;
            padding: 20px;
            min-height: 100vh;
            overflow-x: hidden;
        }

        h2 { 
            margin-bottom: 10px;
            text-shadow: 0 0 10px rgba(0, 195, 255, 0.7);
        }

        .container {
            position: relative;
            border: 6px solid rgba(255, 255, 255, 0.3);
            border-radius: 12px;
            box-shadow: 
                0 15px 35px rgba(0,0,0,0.5),
                inset 0 0 20px rgba(0, 195, 255, 0.3);
            background: black;
            overflow: hidden;
        }

        #videoFeed { display: none; }

        #canvasOut {
            display: block;
            max-width: 100%;
            height: auto;
            border-radius: 6px;
        }

        .controls {
            margin-top: 25px;
            display: flex;
            gap: 15px;
            flex-wrap: wrap;
            justify-content: center;
        }

        button {
            padding: 12px 28px;
            font-size: 16px;
            border: 1px solid rgba(255,255,255,0.4);
            border-radius: 25px;
            cursor: pointer;
            font-weight: bold;
            display: flex;
            align-items: center;
            gap: 8px;
            color: white;
            box-shadow: 0 4px 6px rgba(0,0,0,0.2);
            transition: all 0.2s;
            background-image: linear-gradient(to bottom, rgba(255,255,255,0.3) 0%, rgba(255,255,255,0.1) 50%, rgba(0,0,0,0.1) 51%, rgba(0,0,0,0.2) 100%);
        }
        
        button:hover {
             box-shadow: 0 6px 12px rgba(0, 195, 255, 0.4);
             transform: translateY(-2px);
        }

        #btnStart { background-color: #2ecc71; }
        #btnCapture { background-color: #e74c3c; }
        #btnClear { background-color: #f39c12; }

        #btnCapture:disabled {
            filter: grayscale(80%);
            cursor: not-allowed;
            opacity: 0.7;
        }

        .status {
            margin-top: 15px;
            font-size: 14px;
            color: #89cff0;
            text-shadow: 0 0 5px rgba(0, 195, 255, 0.5);
            height: 20px;
            text-align: center;
        }

        .pulse { animation: pulse-animation 1.5s infinite; }
        @keyframes pulse-animation {
            0% { box-shadow: 0 0 0 0 rgba(238, 82, 83, 0.6); }
            70% { box-shadow: 0 0 0 15px rgba(238, 82, 83, 0); }
            100% { box-shadow: 0 0 0 0 rgba(238, 82, 83, 0); }
        }
    </style>
</head>
<body>

    <h2>Aero Speech Cam (Multi-User)</h2>
    
    <div class="container">
        <video id="videoFeed" playsinline muted></video>
        <canvas id="canvasOut"></canvas>
    </div>

    <div class="status" id="statusMsg">Waiting to start...</div>

    <div class="controls">
        <button id="btnStart" onclick="startApp()">ðŸŸ¢ Start System</button>
        <button id="btnClear" onclick="clearText()">ðŸ§¹ Clear Text</button>
        <button id="btnCapture" onclick="captureMoment()" disabled>ðŸ“¸ Capture Moment</button>
    </div>

    <script>
        // --- Configuration ---
        const canvas = document.getElementById('canvasOut');
        const ctx = canvas.getContext('2d');
        const video = document.getElementById('videoFeed');
        const statusMsg = document.getElementById('statusMsg');
        const btnCapture = document.getElementById('btnCapture');
        const btnStart = document.getElementById('btnStart');

        // State Variables
        let isRunning = false;
        let isRecording = false;
        let currentText = "Bring a friend into frame!";
        let stream = null;
        let recognition = null;
        let mediaRecorder = null;
        let recordedChunks = [];
        let animationId;
        
        // Multi-Face Tracking State
        let faceMesh = null;
        let activeSpeakerIndex = -1; // -1 means no one specifically
        let lastKnownSpeakerIndex = 0; // Default to first person found
        let faces = []; // Store simplified face data
        
        // --- 1. Initialization ---
        async function startApp() {
            btnStart.disabled = true;
            statusMsg.innerText = "Loading AI Models...";

            try {
                faceMesh = new FaceMesh({locateFile: (file) => {
                    return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
                }});
                
                faceMesh.setOptions({
                    maxNumFaces: 4, // Track up to 4 people
                    refineLandmarks: true,
                    minDetectionConfidence: 0.5,
                    minTrackingConfidence: 0.5
                });

                faceMesh.onResults(onFaceResults);

                const camera = new Camera(video, {
                    onFrame: async () => {
                        if(isRunning && !isRecording) {
                            await faceMesh.send({image: video});
                        }
                    },
                    width: 1280,
                    height: 720
                });
                
                // Get Audio
                const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                stream = audioStream;
                
                await camera.start();

                canvas.width = 1280;
                canvas.height = 720;

                setupSpeech();

                isRunning = true;
                btnStart.style.display = 'none';
                btnCapture.disabled = false;
                statusMsg.innerText = "Active. Tracking speakers...";
                
                drawLoop(); 

            } catch (err) {
                console.error(err);
                statusMsg.innerText = "Error initializing. Check console.";
                btnStart.disabled = false;
            }
        }

        function onFaceResults(results) {
            faces = [];
            let maxMouthOpenness = 0;
            let currentBestSpeaker = -1;

            if (results.multiFaceLandmarks) {
                // Iterate over ALL detected faces
                results.multiFaceLandmarks.forEach((landmarks, index) => {
                    // 1. Calculate Mouth Center
                    const topLip = landmarks[13];
                    const bottomLip = landmarks[14];
                    const mouthX = topLip.x * canvas.width;
                    const mouthY = topLip.y * canvas.height;

                    // 2. Calculate "Openness" (Visual Voice Activity Detection)
                    // We measure distance between upper lip (13) and lower lip (14)
                    const openness = Math.abs(topLip.y - bottomLip.y);

                    faces.push({
                        id: index,
                        x: mouthX,
                        y: mouthY,
                        openness: openness
                    });

                    // Check if this person is speaking more than anyone else
                    // 0.01 is a sensitivity threshold (adjust if needed)
                    if (openness > 0.015 && openness > maxMouthOpenness) {
                        maxMouthOpenness = openness;
                        currentBestSpeaker = index;
                    }
                });
            }

            // Update global active speaker if we found a clear candidate
            if (currentBestSpeaker !== -1) {
                activeSpeakerIndex = currentBestSpeaker;
                lastKnownSpeakerIndex = currentBestSpeaker;
            } else {
                // If no one is clearly opening their mouth, stay with last known or none
                activeSpeakerIndex = -1; 
            }

            if (!isRecording) {
                drawScene(results.image);
            }
        }

        // --- 2. Drawing Logic ---
        function drawScene(videoImage) {
            ctx.clearRect(0,0, canvas.width, canvas.height);
            ctx.drawImage(videoImage, 0, 0, canvas.width, canvas.height);

            // Scanline effect
            ctx.fillStyle = 'rgba(0, 100, 255, 0.03)';
            ctx.fillRect(0, 0, canvas.width, canvas.height);

            if (currentText && currentText.trim() !== "") {
                // Decide who gets the bubble
                let targetFace = null;

                // 1. Try to find the currently talking person
                if (activeSpeakerIndex !== -1 && faces[activeSpeakerIndex]) {
                    targetFace = faces[activeSpeakerIndex];
                } 
                // 2. Fallback: Use the last person who spoke
                else if (lastKnownSpeakerIndex !== -1 && faces[lastKnownSpeakerIndex]) {
                    targetFace = faces[lastKnownSpeakerIndex];
                }
                // 3. Fallback: Use the first person available
                else if (faces.length > 0) {
                    targetFace = faces[0];
                }

                if (targetFace) {
                    drawSideBubble(ctx, currentText, targetFace.x, targetFace.y);
                } else {
                    // No faces found? Center it.
                    drawSideBubble(ctx, currentText, canvas.width/2, canvas.height - 100);
                }
            }
        }

        function drawLoop() {
             if(isRecording) return; 
             animationId = requestAnimationFrame(drawLoop);
        }

        function clearText() {
            currentText = "";
            if(recognition) try { recognition.stop(); } catch(e){}
            setTimeout(() => { if(isRunning && !isRecording) recognition.start(); }, 200);
        }

        // --- 3. Bubble Logic (Reused & Optimized) ---
        function drawSideBubble(ctx, text, mouthX, mouthY) {
            ctx.font = "600 32px 'Segoe UI', Roboto, sans-serif";
            const maxWidth = 450;
            const lineHeight = 40;
            
            const words = text.split(' ');
            let line = '';
            let lines = [];
            for(let n = 0; n < words.length; n++) {
                let testLine = line + words[n] + ' ';
                let metrics = ctx.measureText(testLine);
                if (metrics.width > maxWidth && n > 0) {
                    lines.push(line);
                    line = words[n] + ' ';
                } else {
                    line = testLine;
                }
            }
            lines.push(line);

            const paddingH = 30;
            const paddingV = 25;
            let maxLineWidth = 0;
            lines.forEach(l => {
                let w = ctx.measureText(l).width;
                if(w > maxLineWidth) maxLineWidth = w;
            });
            const bWidth = maxLineWidth + (paddingH * 2);
            const bHeight = (lines.length * lineHeight) + (paddingV * 2);

            const offsetDist = 180;
            let bX, bY;
            let onRightSide = true;

            // Determine side based on screen position
            if (mouthX > canvas.width / 2) {
                bX = mouthX - offsetDist - bWidth;
                onRightSide = false;
            } else {
                bX = mouthX + offsetDist;
                onRightSide = true;
            }

            bY = mouthY - bHeight + 50;

            // Clamp
            if (bY < 20) bY = 20;
            if (bY + bHeight > canvas.height - 20) bY = canvas.height - bHeight - 20;
            if (bX < 20) bX = 20;
            if (bX + bWidth > canvas.width - 20) bX = canvas.width - bWidth - 20;

            const cornerRadius = 30;

            ctx.save();
            ctx.shadowColor = 'rgba(0, 195, 255, 0.8)';
            ctx.shadowBlur = 25;
            ctx.shadowOffsetY = 5;

            ctx.beginPath();
            ctx.roundRect(bX, bY, bWidth, bHeight, cornerRadius);

            let mainGrad = ctx.createLinearGradient(bX, bY, bX, bY + bHeight);
            mainGrad.addColorStop(0, 'rgba(235, 250, 255, 0.95)');   
            mainGrad.addColorStop(0.5, 'rgba(200, 240, 255, 0.85)'); 
            mainGrad.addColorStop(1, 'rgba(160, 220, 255, 0.95)');    
            ctx.fillStyle = mainGrad;
            ctx.fill();

            // Tail
            ctx.beginPath();
            if (onRightSide) {
                let tailStartY = Math.min(bY + bHeight - 30, Math.max(bY + 30, mouthY));
                ctx.moveTo(bX + 2, tailStartY - 15); 
                ctx.quadraticCurveTo(mouthX + 40, mouthY, mouthX, mouthY); 
                ctx.quadraticCurveTo(mouthX + 50, mouthY + 20, bX + 2, tailStartY + 15);
            } else {
                let tailStartY = Math.min(bY + bHeight - 30, Math.max(bY + 30, mouthY));
                ctx.moveTo(bX + bWidth - 2, tailStartY - 15); 
                ctx.quadraticCurveTo(mouthX - 40, mouthY, mouthX, mouthY);
                ctx.quadraticCurveTo(mouthX - 50, mouthY + 20, bX + bWidth - 2, tailStartY + 15);
            }
            ctx.fillStyle = mainGrad;
            ctx.fill();

            // Highlight
            ctx.shadowColor = 'transparent';
            const highlightHeight = bHeight * 0.45;
            ctx.beginPath();
            ctx.roundRect(bX + 5, bY + 5, bWidth - 10, highlightHeight, {
                topLeft: cornerRadius - 5, topRight: cornerRadius - 5, bottomLeft: 10, bottomRight: 10
            });
            let highlightGrad = ctx.createLinearGradient(bX, bY, bX, bY + highlightHeight);
            highlightGrad.addColorStop(0, 'rgba(255, 255, 255, 0.9)'); 
            highlightGrad.addColorStop(1, 'rgba(255, 255, 255, 0.1)');
            ctx.fillStyle = highlightGrad;
            ctx.fill();

            // Stroke
            ctx.lineWidth = 3;
            ctx.strokeStyle = 'rgba(100, 200, 255, 0.6)';
            ctx.beginPath();
            ctx.roundRect(bX, bY, bWidth, bHeight, cornerRadius);
            ctx.stroke();

            ctx.restore();

            // Text
            ctx.fillStyle = '#003f5c'; 
            ctx.textAlign = 'center';
            ctx.textBaseline = 'top';
            ctx.shadowColor = 'rgba(255,255,255,1)';
            ctx.shadowBlur = 4;

            const textCenterX = bX + (bWidth / 2);
            const totalTextHeight = lines.length * lineHeight;
            const startTextY = bY + (bHeight - totalTextHeight)/2 + 2;

            for (let k = 0; k < lines.length; k++) {
                ctx.fillText(lines[k], textCenterX, startTextY + (k * lineHeight));
            }
        }

        // --- 4. Speech Logic ---
        function setupSpeech() {
            if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) return;
            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
            recognition = new SpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';

            recognition.onresult = (event) => {
                let interimTranscript = '';
                for (let i = event.resultIndex; i < event.results.length; ++i) {
                    if (event.results[i].isFinal) {
                        currentText = event.results[i][0].transcript;
                    } else {
                        interimTranscript += event.results[i][0].transcript;
                    }
                }
                if (interimTranscript) currentText = interimTranscript;
            };
            
            recognition.onend = () => { if(isRunning && !isRecording) try{recognition.start()}catch(e){} };
            recognition.start();
        }

        // --- 5. Recording Logic (Same as before) ---
        function captureMoment() {
            if (isRecording) return;
            isRecording = true;
            
            if(recognition) recognition.stop();

            btnCapture.innerHTML = "Recording...";
            btnCapture.classList.add("pulse");
            statusMsg.innerText = "Capturing...";

            const canvasStream = canvas.captureStream(30); 
            const audioTrack = stream.getAudioTracks()[0];
            if(audioTrack) canvasStream.addTrack(audioTrack);

            recordedChunks = [];
            let options = { mimeType: 'video/webm; codecs=vp9,opus' };
            if (!MediaRecorder.isTypeSupported(options.mimeType)) options = { mimeType: 'video/webm' };

            mediaRecorder = new MediaRecorder(canvasStream, options);
            mediaRecorder.ondataavailable = (e) => { if (e.data.size > 0) recordedChunks.push(e.data); };
            mediaRecorder.onstop = saveVideo;
            mediaRecorder.start();

            setTimeout(() => { stopCapture(); }, 5000);
        }

        function stopCapture() {
            if(mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop();
            btnCapture.classList.remove("pulse");
            btnCapture.innerHTML = "Processing...";
        }

        function saveVideo() {
            const blob = new Blob(recordedChunks, { type: 'video/webm' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            document.body.appendChild(a);
            a.style = 'display: none';
            a.href = url;
            a.download = `AeroMultiUser_${Date.now()}.webm`; 
            a.click();
            window.URL.revokeObjectURL(url);

            statusMsg.innerText = "Saved.";
            btnCapture.innerHTML = "ðŸ“¸ Capture Moment";
            isRecording = false;
            if(recognition) try{recognition.start()}catch(e){}
        }

    </script>
</body>
</html>
